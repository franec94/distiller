# Scheduler for training / re-training a model using quantization aware training, with a linear, range-based quantizer
#
# The setting here is 8-bit weights (and activations, even biases if are required). For vision models, this is usually applied to the entire model,
# without exceptions. Hence, this scheduler isn't model-specific as-is. It doesn't define any name-based overrides.
#
# At the moment this quantizer will:
#  * Quantize weights and biases for all convolution and FC layers
#  * Quantize all ReLU activations

quantizers:
  linear_quantizer:
    class: QuantAwareTrainRangeLinearQuantizer
    bits_activations: 32
    bits_weights: 8
    bits_bias: 32
    mode: 'SYMMETRIC'  # Can try "ASYMMETRIC_UNSIGNED" as well
    ema_decay: 0.999   # Decay value for exponential moving average tracking of activation ranges
    per_channel_wts: True
    overrides:
    # Don't quantize first and last layer
      net.0.linear:
        bits_activations: null
        bits_weights: null
        bits_bias: null
      net.1.linear:
        bits_weights: 8
        bits_bias: null
        bits_activations: null
      net.2.linear:
        bits_weights: 8
        bits_activations: null
        bits_bias: null
      net.3.linear:
        bits_weights: 8
        bits_activations: null
        bits_bias: null
      net.4.linear:
        bits_weights: 8
        bits_activations: null
        bits_bias: null
      net.5.linear:
        bits_weights: 8
        bits_activations: null
        bits_bias: null
      net.6:
        bits_weights: null
        bits_bias: null
        bits_activations: null

policies:
    - quantizer:
        instance_name: linear_quantizer
      # For now putting a large range here, which should cover both training from scratch or resuming from some
      # pre-trained checkpoint at some unknown epoch
      starting_epoch: 0
      ending_epoch: 440729
      frequency: 25