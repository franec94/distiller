{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "distiller-siren-test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ty3yV03Q3vy0",
        "nOurhnGJ34XX",
        "E_XNWOFb3-uK",
        "OCHyLaOf31nt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMq0gvaI3sKT"
      },
      "source": [
        "## Setup Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty3yV03Q3vy0"
      },
      "source": [
        "#### Options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJLHzM0-tLUE"
      },
      "source": [
        "#@title Enable prune mode\n",
        "#@markdown ---\n",
        "CLONE_GITHUB_PROJECT = True #@param {type:\"boolean\"}\n",
        "INSTALL_SOME_LIBS = True #@param {type:\"boolean\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNyhWxxOZgJw"
      },
      "source": [
        "RUN_COLAB_CODE = False #@param {type:\"boolean\"}\n",
        "RUN_MAIN_SIREN_BASE = False #@param {type:\"boolean\"}\n",
        "RUN_MAIN_SIREN_APP = True #@param {type:\"boolean\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYiSuEnGsaPU"
      },
      "source": [
        "#@title Github related infos\n",
        "#@markdown ---\n",
        "PROJECT_NAME_PATH = '/content/distiller' #@param {type:\"string\"}\n",
        "\n",
        "    \n",
        "GITHUB_PROJECT_URL = 'https://github.com/franec94/distiller.git' #@param {type:\"string\"}\n",
        "BRANCH_NAME = 'siren-support' #@param {type:\"string\"}\n",
        "CMD_TOOL_NAME = '' #@param {type:\"string\"}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOurhnGJ34XX"
      },
      "source": [
        "### Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn8rIEwlvBH0",
        "outputId": "8f99fe9e-465a-4b74-d2c9-cd14e6dea927"
      },
      "source": [
        "# Installing third party dependencies\n",
        "if INSTALL_SOME_LIBS:\n",
        "    print(\"Installing required libraries...\")\n",
        "\n",
        "    old_requirements = '/content/tmp_requirements.txt'\n",
        "    !pip freeze > {old_requirements}\n",
        "    dependencies_list = \"pretrainedmodels,torchnet,xlsxwriter,gitpython,python-git,cmapy,sk-video,pytorch-model-summary,ConfigArgParse,tabulate,chart_studio,dash,dash_bootstrap_components\".split(\",\")\n",
        "\n",
        "    with open(old_requirements) as f:\n",
        "        old_requirements_list = f.read().split(\"\\n\")\n",
        "        for a_req in dependencies_list:\n",
        "            found_req = False\n",
        "            for old_req in old_requirements_list:\n",
        "                if old_req.startswith(a_req):\n",
        "                    print(f\"{a_req} already installed!\")\n",
        "                    found_req = True\n",
        "                    break\n",
        "            if found_req is False:\n",
        "                !pip install {a_req} -q\n",
        "        pass\n",
        "    !rm -f {old_requirements}\n",
        "    pass"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing required libraries...\n",
            "pretrainedmodels already installed!\n",
            "torchnet already installed!\n",
            "python-git already installed!\n",
            "cmapy already installed!\n",
            "sk-video already installed!\n",
            "pytorch-model-summary already installed!\n",
            "ConfigArgParse already installed!\n",
            "tabulate already installed!\n",
            "dash already installed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnXrYy2C4pSJ"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "if RUN_COLAB_CODE:\n",
        "\n",
        "\n",
        "    # --------------------------------------------- #\n",
        "    # Standard Library, plus some Third Party Libraries\n",
        "    # --------------------------------------------- #\n",
        "\n",
        "    DASH_TEMPLATES_LIST = [\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]\n",
        "\n",
        "    from PIL import Image\n",
        "    from functools import partial\n",
        "    from pprint import pprint\n",
        "    from tqdm import tqdm\n",
        "    from typing import Tuple, Union\n",
        "\n",
        "\n",
        "    import configargparse\n",
        "    import copy\n",
        "    import collections\n",
        "    import datetime\n",
        "    import functools\n",
        "    import itertools\n",
        "    import h5py\n",
        "    import logging\n",
        "    import math\n",
        "    import os\n",
        "    import operator\n",
        "    import pickle\n",
        "    import random\n",
        "    import shutil\n",
        "    import sys\n",
        "    import re\n",
        "    import tabulate \n",
        "    import time\n",
        "    # import visdom\n",
        "\n",
        "\n",
        "    from collections import OrderedDict\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # --------------------------------------------- #\n",
        "    # Data Science and Machine Learning Libraries\n",
        "    # --------------------------------------------- #\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    matplotlib.style.use('ggplot')\n",
        "    import seaborn as sns\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import sklearn\n",
        "\n",
        "    from sklearn.model_selection import ParameterGrid\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # --------------------------------------------- #\n",
        "    # Torch\n",
        "    # --------------------------------------------- #\n",
        "    import torch\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        import torch.nn.functional as F\n",
        "        import torch.optim as optim\n",
        "        from torch.utils.data import DataLoader, Dataset\n",
        "        # import torch.quantization\n",
        "        # import torch.nn.utils.prune as prune\n",
        "\n",
        "        from torch import nn, optim\n",
        "\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        print(\"torch not available!\")\n",
        "        pass\n",
        "\n",
        "    from numpy import linalg as LA\n",
        "    from scipy.stats import rankdata\n",
        "    from collections import OrderedDict\n",
        "    from torchvision import datasets, transforms\n",
        "    from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "    # --------------------------------------------- #\n",
        "    # Import: torch_pruning\n",
        "    # --------------------------------------------- #\n",
        "    # import torch_pruning as tp\n",
        "\n",
        "\n",
        "    # --------------------------------------------- #\n",
        "    # Import: TorchVision\n",
        "    # --------------------------------------------- #\n",
        "    try:\n",
        "        import torchvision\n",
        "        from torchvision import datasets\n",
        "        from torchvision import transforms\n",
        "        from torchvision.transforms import Resize, Compose, ToTensor, CenterCrop, Normalize\n",
        "        from torchvision.utils import save_image\n",
        "    except:\n",
        "        print(\"torchvision library not available!\")\n",
        "        pass\n",
        "\n",
        "    # Plotly imports.\n",
        "    # ----------------------------------------------- #\n",
        "    import chart_studio.plotly as py\n",
        "    import plotly.figure_factory as ff\n",
        "    import plotly.express as px\n",
        "\n",
        "    # --------------------------------------------- #\n",
        "    # Import: skimage\n",
        "    # --------------------------------------------- #\n",
        "    try:\n",
        "        import skimage\n",
        "        import skimage.metrics as skmetrics\n",
        "        from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "        from skimage.metrics import structural_similarity as ssim\n",
        "        from skimage.metrics import mean_squared_error\n",
        "    except:\n",
        "        print(\"skimage library not available!\")\n",
        "        pass"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_XNWOFb3-uK"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHQwNqhL4Djc"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    class SineLayer(nn.Module):\n",
        "        # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "        \n",
        "        # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the \n",
        "        # nonlinearity. Different signals may require different omega_0 in the first layer - this is a \n",
        "        # hyperparameter.\n",
        "        \n",
        "        # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of \n",
        "        # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "        \n",
        "        def __init__(self, in_features, out_features, bias=True,\n",
        "                    is_first=False, omega_0=30):\n",
        "            super().__init__()\n",
        "            self.omega_0 = omega_0\n",
        "            self.is_first = is_first\n",
        "            \n",
        "            self.in_features = in_features\n",
        "            self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "            \n",
        "            self.init_weights()\n",
        "            pass\n",
        "        \n",
        "        def init_weights(self):\n",
        "            with torch.no_grad():\n",
        "                if self.is_first:\n",
        "                    self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                                1 / self.in_features)      \n",
        "                else:\n",
        "                    self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                                np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "            pass\n",
        "            \n",
        "        def forward(self, input):\n",
        "            return torch.sin(self.omega_0 * self.linear(input))\n",
        "        \n",
        "        def forward_with_intermediate(self, input): \n",
        "            # For visualization of activation distributions\n",
        "            intermediate = self.omega_0 * self.linear(input)\n",
        "            return torch.sin(intermediate), intermediate\n",
        "        pass\n",
        "        \n",
        "        \n",
        "    class Siren(nn.Module):\n",
        "        def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                    first_omega_0=30, hidden_omega_0=30.):\n",
        "            super().__init__()\n",
        "            \n",
        "            self.net = []\n",
        "            self.net.append(SineLayer(in_features, hidden_features, \n",
        "                                    is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "            for i in range(hidden_layers):\n",
        "                self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                        is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "            if outermost_linear:\n",
        "                final_linear = nn.Linear(hidden_features, out_features)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                                np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                    \n",
        "                self.net.append(final_linear)\n",
        "            else:\n",
        "                self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                        is_first=False, omega_0=hidden_omega_0))\n",
        "            \n",
        "            self.net = nn.Sequential(*self.net)\n",
        "            pass\n",
        "        \n",
        "        def forward(self, coords):\n",
        "            coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "            output = self.net(coords)\n",
        "            return output, coords        \n",
        "\n",
        "        def forward_with_activations(self, coords, retain_grad=False):\n",
        "            '''Returns not only model output, but also intermediate activations.\n",
        "            Only used for visualizing activations later!'''\n",
        "            activations = OrderedDict()\n",
        "\n",
        "            activation_count = 0\n",
        "            x = coords.clone().detach().requires_grad_(True)\n",
        "            activations['input'] = x\n",
        "            for i, layer in enumerate(self.net):\n",
        "                if isinstance(layer, SineLayer):\n",
        "                    x, intermed = layer.forward_with_intermediate(x)\n",
        "                    \n",
        "                    if retain_grad:\n",
        "                        x.retain_grad()\n",
        "                        intermed.retain_grad()\n",
        "                        \n",
        "                    activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                    activation_count += 1\n",
        "                else: \n",
        "                    x = layer(x)\n",
        "                    \n",
        "                    if retain_grad:\n",
        "                        x.retain_grad()\n",
        "                        \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "                activation_count += 1\n",
        "\n",
        "            return activations\n",
        "        pass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCHyLaOf31nt"
      },
      "source": [
        "### Clone Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Xg5uZOuBh6"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P0EpvmRr5A2",
        "outputId": "9022fa7b-3a34-44f2-8acf-2d2be23879c3"
      },
      "source": [
        "# Setup wd to remove trash\n",
        "\n",
        "if CLONE_GITHUB_PROJECT:\n",
        "    # Remove trash\n",
        "    import os\n",
        "    if os.path.exists(f'{PROJECT_NAME_PATH}') and os.path.isdir(f'{PROJECT_NAME_PATH}'):\n",
        "        print(\"Cleaning from old project...\")\n",
        "        !rm -r {PROJECT_NAME_PATH}\n",
        "    else:\n",
        "        print(\"No project found.\")\n",
        "        pass\n",
        "\n",
        "    import os\n",
        "    logs_base_dir = os.path.join(\"/content/outputs\", \"summaries\")\n",
        "    os.makedirs(logs_base_dir, exist_ok=True)\n",
        "\n",
        "    if os.path.exists(PROJECT_NAME_PATH) is False:\n",
        "        !git clone {GITHUB_PROJECT_URL}\n",
        "        os.chdir(PROJECT_NAME_PATH)\n",
        "        !git checkout {BRANCH_NAME}\n",
        "        if CMD_TOOL_NAME == None or len(CMD_TOOL_NAME) == 0:\n",
        "            full_path_cmd = os.path.join(PROJECT_NAME_PATH, f'{PROJECT_NAME_PATH}/')\n",
        "            os.chdir(full_path_cmd)\n",
        "        else:\n",
        "            full_path_cmd = os.path.join(PROJECT_NAME_PATH, f'{PROJECT_NAME_PATH}/{CMD_TOOL_NAME}')\n",
        "            os.chdir(full_path_cmd)\n",
        "    else:\n",
        "        os.chdir(PROJECT_NAME_PATH)\n",
        "        !git checkout {BRANCH_NAME}\n",
        "        !git fetch\n",
        "        if CMD_TOOL_NAME == None or len(CMD_TOOL_NAME) == 0:\n",
        "            full_path_cmd = os.path.join(PROJECT_NAME_PATH, f'{PROJECT_NAME_PATH}/')\n",
        "            os.chdir(full_path_cmd)\n",
        "        else:\n",
        "            full_path_cmd = os.path.join(PROJECT_NAME_PATH, f'{PROJECT_NAME_PATH}/{CMD_TOOL_NAME}')\n",
        "            os.chdir(full_path_cmd)\n",
        "        pass\n",
        "    pass\n",
        "else:\n",
        "    print(\"No github project cloned and no branch activated and switched to!\")\n",
        "    pass"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning from old project...\n",
            "Cloning into 'distiller'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 6436 (delta 0), reused 1 (delta 0), pack-reused 6430\u001b[K\n",
            "Receiving objects: 100% (6436/6436), 39.87 MiB | 34.87 MiB/s, done.\n",
            "Resolving deltas: 100% (4523/4523), done.\n",
            "Branch 'siren-support' set up to track remote branch 'siren-support' from 'origin'.\n",
            "Switched to a new branch 'siren-support'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GoBmT2WuRUp",
        "outputId": "010eaee1-9a41-40c1-f597-66f864b7fa97"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/distiller\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNVhhr_jy8gZ",
        "outputId": "2f4f7fa8-b544-41a1-aadd-a152bea38af7"
      },
      "source": [
        "# !pip install -e .\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow<7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (6.2.2)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.18.5)\n",
            "Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: gitpython==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: torchnet==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.0.4)\n",
            "Requirement already satisfied: tensorflow~=1.14 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.15.4)\n",
            "Requirement already satisfied: pydot==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: tabulate==0.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.8.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (1.1.4)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (3.2.2)\n",
            "Requirement already satisfied: qgrid==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (1.1.1)\n",
            "Requirement already satisfied: graphviz==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.10.1)\n",
            "Requirement already satisfied: ipywidgets==7.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (7.4.2)\n",
            "Requirement already satisfied: bqplot==0.11.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (0.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: pytest~=4.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (4.6.11)\n",
            "Requirement already satisfied: xlsxwriter>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (1.3.7)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (0.7.4)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (0.21.2)\n",
            "Requirement already satisfied: gym==0.12.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (0.12.5)\n",
            "Requirement already satisfied: tqdm==4.33.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 24)) (4.33.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython==3.1.0->-r requirements.txt (line 6)) (4.0.5)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.6/dist-packages (from torchnet==0.0.4->-r requirements.txt (line 7)) (0.1.8.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.33.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.15.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (0.35.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=1.14->-r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot==1.4.1->-r requirements.txt (line 9)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->-r requirements.txt (line 11)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->-r requirements.txt (line 11)) (2018.9)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 12)) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 12)) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 12)) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 12)) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 12)) (4.7.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib~=3.0->-r requirements.txt (line 13)) (0.10.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets==7.4.2->-r requirements.txt (line 16)) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets==7.4.2->-r requirements.txt (line 16)) (3.4.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets==7.4.2->-r requirements.txt (line 16)) (4.3.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets==7.4.2->-r requirements.txt (line 16)) (5.0.8)\n",
            "Requirement already satisfied: traittypes>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from bqplot==0.11.5->-r requirements.txt (line 17)) (0.2.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (20.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (0.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (20.4)\n",
            "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (8.6.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (1.4.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest~=4.6.1->-r requirements.txt (line 19)) (2.0.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4->-r requirements.txt (line 21)) (2.5.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->-r requirements.txt (line 22)) (0.17.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.12.5->-r requirements.txt (line 23)) (1.5.0)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython==3.1.0->-r requirements.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (0.57.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (20.0.0)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (0.1.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (5.1.1)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (1.27)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow~=1.14->-r requirements.txt (line 8)) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow~=1.14->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow~=1.14->-r requirements.txt (line 8)) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow~=1.14->-r requirements.txt (line 8)) (50.3.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 12)) (5.3.5)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (4.7.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (1.4.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (2.11.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 12)) (1.0.18)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 12)) (1.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter>=1.0.0->-r requirements.txt (line 12)) (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.4.2->-r requirements.txt (line 16)) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.4.2->-r requirements.txt (line 16)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.4.2->-r requirements.txt (line 16)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets==7.4.2->-r requirements.txt (line 16)) (0.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets==7.4.2->-r requirements.txt (line 16)) (2.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest~=4.6.1->-r requirements.txt (line 19)) (3.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.12.5->-r requirements.txt (line 23)) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->visdom->torchnet==0.0.4->-r requirements.txt (line 7)) (2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 12)) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 12)) (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKA9cO1r37po"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rka87pahCXA"
      },
      "source": [
        "### Colab mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0D4tOEuz0M3"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/distiller')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc-IKCFGSHfp"
      },
      "source": [
        "import distiller"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MjxpEPzGykl6",
        "outputId": "078d1c6d-6c05-40be-c49f-bc33ec2dad29"
      },
      "source": [
        "distiller.__version__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Unknown'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7zHz_fr2Kx"
      },
      "source": [
        "import os\n",
        "import torch.nn as nn\n",
        "from distiller.models import register_user_model\n",
        "import distiller.apputils.siren_image_regressor as regressor"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUlcZfeX4VgJ"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    def siren_model_64_5():\n",
        "        img_siren = Siren(in_features=2, out_features=1, hidden_features=64, \n",
        "                  hidden_layers=5, outermost_linear=True)\n",
        "        return img_siren"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrpGKnfo3SMH"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    distiller.models.register_user_model(arch=\"SirenModel_64_5\", dataset=\"cameramen\", model=siren_model_64_5)\n",
        "    model = distiller.models.create_model(pretrained=False, dataset=\"cameramen\", arch=\"SirenModel_64_5\")\n",
        "    assert model is not None"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2QCI3osLupC"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    for name, module in model.named_modules():\n",
        "        print(name)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZEY8G1O5Pm9"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    def init_jupyter_default_args(args):\n",
        "        args.output_dir = '/content/' # None # \n",
        "        args.evaluate = False\n",
        "        args.seed = 0\n",
        "        args.deterministic = True\n",
        "        args.cpu = False\n",
        "        args.gpus = \"0\"\n",
        "        args.load_serialized = False\n",
        "        args.deprecated_resume = None\n",
        "        args.resumed_checkpoint_path = None\n",
        "        args.load_model_path = None\n",
        "        args.reset_optimizer = False\n",
        "        args.lr = args.momentum = args.weight_decay = 0.\n",
        "        args.compress = '/content/distiller/examples/agp-pruning/siren64-5_schedule_agp.yaml'\n",
        "        args.epochs = 0\n",
        "        args.activation_stats = list()\n",
        "        args.batch_size = 1\n",
        "        args.workers = 1\n",
        "        args.validation_split = 0.1\n",
        "        args.effective_train_size = args.effective_valid_size = args.effective_test_size = 1.\n",
        "        args.log_params_histograms = False\n",
        "        args.print_freq = 10\n",
        "        args.masks_sparsity = False\n",
        "        args.display_confusion = False\n",
        "        args.num_best_scores = 1\n",
        "        args.name = \"\"\n",
        "        args.kd_policy = None\n",
        "        # args.summary = \"sparsity\"\n",
        "        args.qe_stats_file = None\n",
        "        args.verbose = False\n",
        "        return args\n",
        "\n",
        "\n",
        "    def config_learner_args(args, arch, dataset, dataset_path, pretrained, adam_args, batch, epochs):\n",
        "        args.arch = f\"{arch}\"\n",
        "        args.dataset = f\"{dataset}\"\n",
        "        args.data = \"\"\n",
        "        args.pretrained = False\n",
        "        args.lr = adam_args[0]\n",
        "        args.momentum = adam_args[1]\n",
        "        args.weight_decay = adam_args[2]\n",
        "        args.batch_size = 1\n",
        "        args.epochs = epochs\n",
        "        return args"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OlAJI5AflcY"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    args = regressor.init_regressor_compression_arg_parser()\n",
        "    args, unknownargs = args.parse_known_args()\n",
        "    pprint(args)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5oYcZdH6Gdq"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    args = init_jupyter_default_args(args)\n",
        "    args.batch_size"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUN--sR1fuy8"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    args = config_learner_args(args, \"SirenModel_64_5\", \"cameramen\", \"\", False, (0.1, 0.0, 1e-4) , 1, 100)\n",
        "    args.arch, args.epochs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkgrmlFnaGeA"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    args.arch, args.verbose, args.print_freq"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7RZuDgC6kcw"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    app = regressor.SirenRegressorCompressor(args, script_dir=os.path.dirname(\".\"))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tMAe4gRM3lm"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# %reload_ext tensorboard"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb2FswBZPpzx"
      },
      "source": [
        "# !kill 6966"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVT-QdsFM6-P"
      },
      "source": [
        "# %tensorboard --logdir /content/logs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGiV2QryRppS"
      },
      "source": [
        "# Run the training loop\n",
        "if RUN_COLAB_CODE:\n",
        "    perf_scores_history = app.run_training_loop()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5geXn_5jLYZm"
      },
      "source": [
        "if RUN_COLAB_CODE:\n",
        "    print(perf_scores_history[-1])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSMDhVWvhFDx"
      },
      "source": [
        "### Base mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjrHhdPS6O_"
      },
      "source": [
        "if RUN_MAIN_SIREN_BASE:\n",
        "    !python main.py \\\n",
        "        --logging_root '/content/results/cameramen' \\\n",
        "        --experiment_name 'train' \\\n",
        "        --sidelength 256 \\\n",
        "        --num_epochs 100 \\\n",
        "        --n_hf 64  \\\n",
        "        --n_hl 5 \\\n",
        "        --lambda_L_1 0 \\\n",
        "        --lambda_L_2 0.0001 \\\n",
        "        --epochs_til_ckpt 10 \\\n",
        "        --seed 0 \\\n",
        "        --cuda \\\n",
        "        --train \\\n",
        "        --evaluate \\\n",
        "        --dynamic_quant qint8 qfloat16 \\\n",
        "        --verbose 0\n",
        "    pass"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXW6Y5zwhGp5"
      },
      "source": [
        "### App Mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BWb3M1eXm3m",
        "outputId": "0e94495c-3404-4855-b7b8-8e7015728938"
      },
      "source": [
        "if RUN_MAIN_SIREN_APP:\n",
        "    !python siren_main_app.py \\\n",
        "        --logging_root '/content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen' \\\n",
        "        --experiment_name 'train' \\\n",
        "        --sidelength 256 \\\n",
        "        --num_epochs 175000 \\\n",
        "        --n_hf 64  \\\n",
        "        --n_hl 5 \\\n",
        "        --lambda_L_1 0 \\\n",
        "        --lambda_L_2 0.0001 \\\n",
        "        --epochs_til_ckpt 1750 \\\n",
        "        --seed 0 \\\n",
        "        --cuda \\\n",
        "        --train \\\n",
        "        --evaluate \\\n",
        "        --dynamic_quant qint8 qfloat16 \\\n",
        "        --verbose 0\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not find the logger configuration file (/content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/logging.conf) - using default logger configuration\n",
            "Log file for this run: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/___2020.11.27-162900.log\n",
            "Random seed: 0\n",
            "\n",
            "--------------------------------------------------------\n",
            "Logging to TensorBoard - remember to execute the server:\n",
            "> tensorboard --logdir='./logs'\n",
            "\n",
            "=> created a SirenCompressingModel model with the cameramen dataset\n",
            "Dataset sizes:\n",
            "\ttraining=1\n",
            "\tvalidation=1\n",
            "\ttest=1\n",
            "\n",
            "\n",
            "--- train (epoch=0)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [0][    1/    1]    Overall Loss 0.396523    Objective Loss 0.396523    LR 0.000100    Time 0.014904    \n",
            "--- validate (epoch=0)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [0][    1/    1]    Loss 0.363788    \n",
            "==> Loss: 0.3637884\n",
            "\n",
            "==> Best [MSE: 0.3637884   Sparsity:0.00   NNZ-Params: 20672 on epoch: 0]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=1750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [1750][    1/    1]    Overall Loss 0.000428    Objective Loss 0.000428    LR 0.000100    Time 0.007214    \n",
            "--- validate (epoch=1750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [1750][    1/    1]    Loss 0.000425    \n",
            "==> Loss: 0.0004254\n",
            "\n",
            "==> Best [MSE: 0.0004254   Sparsity:0.00   NNZ-Params: 20672 on epoch: 1750]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=3500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [3500][    1/    1]    Overall Loss 0.000215    Objective Loss 0.000215    LR 0.000100    Time 0.007225    \n",
            "--- validate (epoch=3500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [3500][    1/    1]    Loss 0.000215    \n",
            "==> Loss: 0.0002150\n",
            "\n",
            "==> Best [MSE: 0.0002145   Sparsity:0.00   NNZ-Params: 20672 on epoch: 3496]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=5250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [5250][    1/    1]    Overall Loss 0.000145    Objective Loss 0.000145    LR 0.000100    Time 0.007379    \n",
            "--- validate (epoch=5250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [5250][    1/    1]    Loss 0.000145    \n",
            "==> Loss: 0.0001446\n",
            "\n",
            "==> Best [MSE: 0.0001445   Sparsity:0.00   NNZ-Params: 20672 on epoch: 5247]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=7000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [7000][    1/    1]    Overall Loss 0.000383    Objective Loss 0.000383    LR 0.000100    Time 0.008484    \n",
            "--- validate (epoch=7000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [7000][    1/    1]    Loss 0.000440    \n",
            "==> Loss: 0.0004397\n",
            "\n",
            "==> Best [MSE: 0.0001126   Sparsity:0.00   NNZ-Params: 20672 on epoch: 6983]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=8750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [8750][    1/    1]    Overall Loss 0.000098    Objective Loss 0.000098    LR 0.000100    Time 0.007206    \n",
            "--- validate (epoch=8750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [8750][    1/    1]    Loss 0.000100    \n",
            "==> Loss: 0.0000998\n",
            "\n",
            "==> Best [MSE: 0.0000964   Sparsity:0.00   NNZ-Params: 20672 on epoch: 8625]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=10500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [10500][    1/    1]    Overall Loss 0.000086    Objective Loss 0.000086    LR 0.000100    Time 0.011252    \n",
            "--- validate (epoch=10500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [10500][    1/    1]    Loss 0.000087    \n",
            "==> Loss: 0.0000868\n",
            "\n",
            "==> Best [MSE: 0.0000859   Sparsity:0.00   NNZ-Params: 20672 on epoch: 10325]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=12250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [12250][    1/    1]    Overall Loss 0.000125    Objective Loss 0.000125    LR 0.000100    Time 0.007215    \n",
            "--- validate (epoch=12250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [12250][    1/    1]    Loss 0.000130    \n",
            "==> Loss: 0.0001297\n",
            "\n",
            "==> Best [MSE: 0.0000791   Sparsity:0.00   NNZ-Params: 20672 on epoch: 12165]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=14000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [14000][    1/    1]    Overall Loss 0.000079    Objective Loss 0.000079    LR 0.000100    Time 0.007423    \n",
            "--- validate (epoch=14000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [14000][    1/    1]    Loss 0.000079    \n",
            "==> Loss: 0.0000795\n",
            "\n",
            "==> Best [MSE: 0.0000746   Sparsity:0.00   NNZ-Params: 20672 on epoch: 13737]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=15750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [15750][    1/    1]    Overall Loss 0.000076    Objective Loss 0.000076    LR 0.000100    Time 0.007830    \n",
            "--- validate (epoch=15750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [15750][    1/    1]    Loss 0.000076    \n",
            "==> Loss: 0.0000757\n",
            "\n",
            "==> Best [MSE: 0.0000708   Sparsity:0.00   NNZ-Params: 20672 on epoch: 15694]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=17500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [17500][    1/    1]    Overall Loss 0.000116    Objective Loss 0.000116    LR 0.000100    Time 0.008036    \n",
            "--- validate (epoch=17500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [17500][    1/    1]    Loss 0.000101    \n",
            "==> Loss: 0.0001007\n",
            "\n",
            "==> Best [MSE: 0.0000685   Sparsity:0.00   NNZ-Params: 20672 on epoch: 17105]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=19250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [19250][    1/    1]    Overall Loss 0.000067    Objective Loss 0.000067    LR 0.000100    Time 0.008692    \n",
            "--- validate (epoch=19250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [19250][    1/    1]    Loss 0.000066    \n",
            "==> Loss: 0.0000664\n",
            "\n",
            "==> Best [MSE: 0.0000655   Sparsity:0.00   NNZ-Params: 20672 on epoch: 19089]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=21000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [21000][    1/    1]    Overall Loss 0.000086    Objective Loss 0.000086    LR 0.000100    Time 0.008492    \n",
            "--- validate (epoch=21000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [21000][    1/    1]    Loss 0.000083    \n",
            "==> Loss: 0.0000834\n",
            "\n",
            "==> Best [MSE: 0.0000634   Sparsity:0.00   NNZ-Params: 20672 on epoch: 20960]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=22750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [22750][    1/    1]    Overall Loss 0.000069    Objective Loss 0.000069    LR 0.000100    Time 0.009540    \n",
            "--- validate (epoch=22750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [22750][    1/    1]    Loss 0.000067    \n",
            "==> Loss: 0.0000671\n",
            "\n",
            "==> Best [MSE: 0.0000611   Sparsity:0.00   NNZ-Params: 20672 on epoch: 22655]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=24500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [24500][    1/    1]    Overall Loss 0.000074    Objective Loss 0.000074    LR 0.000100    Time 0.008474    \n",
            "--- validate (epoch=24500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [24500][    1/    1]    Loss 0.000066    \n",
            "==> Loss: 0.0000661\n",
            "\n",
            "==> Best [MSE: 0.0000595   Sparsity:0.00   NNZ-Params: 20672 on epoch: 24211]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=26250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [26250][    1/    1]    Overall Loss 0.000068    Objective Loss 0.000068    LR 0.000100    Time 0.008498    \n",
            "--- validate (epoch=26250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [26250][    1/    1]    Loss 0.000075    \n",
            "==> Loss: 0.0000751\n",
            "\n",
            "==> Best [MSE: 0.0000579   Sparsity:0.00   NNZ-Params: 20672 on epoch: 26207]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=28000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [28000][    1/    1]    Overall Loss 0.000084    Objective Loss 0.000084    LR 0.000100    Time 0.008581    \n",
            "--- validate (epoch=28000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [28000][    1/    1]    Loss 0.000084    \n",
            "==> Loss: 0.0000837\n",
            "\n",
            "==> Best [MSE: 0.0000566   Sparsity:0.00   NNZ-Params: 20672 on epoch: 27656]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=29750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [29750][    1/    1]    Overall Loss 0.000056    Objective Loss 0.000056    LR 0.000100    Time 0.008703    \n",
            "--- validate (epoch=29750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [29750][    1/    1]    Loss 0.000055    \n",
            "==> Loss: 0.0000555\n",
            "\n",
            "==> Best [MSE: 0.0000555   Sparsity:0.00   NNZ-Params: 20672 on epoch: 29750]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=31500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [31500][    1/    1]    Overall Loss 0.000058    Objective Loss 0.000058    LR 0.000100    Time 0.008619    \n",
            "--- validate (epoch=31500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [31500][    1/    1]    Loss 0.000056    \n",
            "==> Loss: 0.0000564\n",
            "\n",
            "==> Best [MSE: 0.0000545   Sparsity:0.00   NNZ-Params: 20672 on epoch: 31449]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=33250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [33250][    1/    1]    Overall Loss 0.000059    Objective Loss 0.000059    LR 0.000100    Time 0.008579    \n",
            "--- validate (epoch=33250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [33250][    1/    1]    Loss 0.000059    \n",
            "==> Loss: 0.0000587\n",
            "\n",
            "==> Best [MSE: 0.0000537   Sparsity:0.00   NNZ-Params: 20672 on epoch: 33144]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=35000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [35000][    1/    1]    Overall Loss 0.000054    Objective Loss 0.000054    LR 0.000100    Time 0.008760    \n",
            "--- validate (epoch=35000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [35000][    1/    1]    Loss 0.000054    \n",
            "==> Loss: 0.0000542\n",
            "\n",
            "==> Best [MSE: 0.0000530   Sparsity:0.00   NNZ-Params: 20672 on epoch: 34936]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=36750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [36750][    1/    1]    Overall Loss 0.000053    Objective Loss 0.000053    LR 0.000100    Time 0.008763    \n",
            "--- validate (epoch=36750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [36750][    1/    1]    Loss 0.000053    \n",
            "==> Loss: 0.0000525\n",
            "\n",
            "==> Best [MSE: 0.0000521   Sparsity:0.00   NNZ-Params: 20672 on epoch: 36576]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=38500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [38500][    1/    1]    Overall Loss 0.000055    Objective Loss 0.000055    LR 0.000100    Time 0.008848    \n",
            "--- validate (epoch=38500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [38500][    1/    1]    Loss 0.000056    \n",
            "==> Loss: 0.0000557\n",
            "\n",
            "==> Best [MSE: 0.0000515   Sparsity:0.00   NNZ-Params: 20672 on epoch: 38116]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=40250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [40250][    1/    1]    Overall Loss 0.000052    Objective Loss 0.000052    LR 0.000100    Time 0.008635    \n",
            "--- validate (epoch=40250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [40250][    1/    1]    Loss 0.000052    \n",
            "==> Loss: 0.0000524\n",
            "\n",
            "==> Best [MSE: 0.0000507   Sparsity:0.00   NNZ-Params: 20672 on epoch: 39875]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=42000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [42000][    1/    1]    Overall Loss 0.000055    Objective Loss 0.000055    LR 0.000100    Time 0.008645    \n",
            "--- validate (epoch=42000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [42000][    1/    1]    Loss 0.000058    \n",
            "==> Loss: 0.0000576\n",
            "\n",
            "==> Best [MSE: 0.0000498   Sparsity:0.00   NNZ-Params: 20672 on epoch: 41946]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=43750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [43750][    1/    1]    Overall Loss 0.000052    Objective Loss 0.000052    LR 0.000100    Time 0.008903    \n",
            "--- validate (epoch=43750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [43750][    1/    1]    Loss 0.000053    \n",
            "==> Loss: 0.0000534\n",
            "\n",
            "==> Best [MSE: 0.0000495   Sparsity:0.00   NNZ-Params: 20672 on epoch: 42579]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=45500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [45500][    1/    1]    Overall Loss 0.000053    Objective Loss 0.000053    LR 0.000100    Time 0.009212    \n",
            "--- validate (epoch=45500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [45500][    1/    1]    Loss 0.000053    \n",
            "==> Loss: 0.0000527\n",
            "\n",
            "==> Best [MSE: 0.0000488   Sparsity:0.00   NNZ-Params: 20672 on epoch: 45322]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=47250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [47250][    1/    1]    Overall Loss 0.000052    Objective Loss 0.000052    LR 0.000100    Time 0.008981    \n",
            "--- validate (epoch=47250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [47250][    1/    1]    Loss 0.000052    \n",
            "==> Loss: 0.0000523\n",
            "\n",
            "==> Best [MSE: 0.0000484   Sparsity:0.00   NNZ-Params: 20672 on epoch: 47117]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=49000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [49000][    1/    1]    Overall Loss 0.000158    Objective Loss 0.000158    LR 0.000100    Time 0.008863    \n",
            "--- validate (epoch=49000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [49000][    1/    1]    Loss 0.000159    \n",
            "==> Loss: 0.0001588\n",
            "\n",
            "==> Best [MSE: 0.0000479   Sparsity:0.00   NNZ-Params: 20672 on epoch: 48883]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=50750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [50750][    1/    1]    Overall Loss 0.000049    Objective Loss 0.000049    LR 0.000100    Time 0.010043    \n",
            "--- validate (epoch=50750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [50750][    1/    1]    Loss 0.000050    \n",
            "==> Loss: 0.0000496\n",
            "\n",
            "==> Best [MSE: 0.0000473   Sparsity:0.00   NNZ-Params: 20672 on epoch: 49896]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=52500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [52500][    1/    1]    Overall Loss 0.000050    Objective Loss 0.000050    LR 0.000100    Time 0.008918    \n",
            "--- validate (epoch=52500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [52500][    1/    1]    Loss 0.000052    \n",
            "==> Loss: 0.0000518\n",
            "\n",
            "==> Best [MSE: 0.0000469   Sparsity:0.00   NNZ-Params: 20672 on epoch: 51244]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=54250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [54250][    1/    1]    Overall Loss 0.000066    Objective Loss 0.000066    LR 0.000100    Time 0.009053    \n",
            "--- validate (epoch=54250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [54250][    1/    1]    Loss 0.000071    \n",
            "==> Loss: 0.0000715\n",
            "\n",
            "==> Best [MSE: 0.0000464   Sparsity:0.00   NNZ-Params: 20672 on epoch: 54209]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=56000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [56000][    1/    1]    Overall Loss 0.000096    Objective Loss 0.000096    LR 0.000100    Time 0.008775    \n",
            "--- validate (epoch=56000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [56000][    1/    1]    Loss 0.000086    \n",
            "==> Loss: 0.0000865\n",
            "\n",
            "==> Best [MSE: 0.0000457   Sparsity:0.00   NNZ-Params: 20672 on epoch: 55830]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=57750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [57750][    1/    1]    Overall Loss 0.000047    Objective Loss 0.000047    LR 0.000100    Time 0.008560    \n",
            "--- validate (epoch=57750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [57750][    1/    1]    Loss 0.000047    \n",
            "==> Loss: 0.0000469\n",
            "\n",
            "==> Best [MSE: 0.0000457   Sparsity:0.00   NNZ-Params: 20672 on epoch: 55830]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=59500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [59500][    1/    1]    Overall Loss 0.000052    Objective Loss 0.000052    LR 0.000100    Time 0.009796    \n",
            "--- validate (epoch=59500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [59500][    1/    1]    Loss 0.000053    \n",
            "==> Loss: 0.0000532\n",
            "\n",
            "==> Best [MSE: 0.0000449   Sparsity:0.00   NNZ-Params: 20672 on epoch: 59213]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=61250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [61250][    1/    1]    Overall Loss 0.000055    Objective Loss 0.000055    LR 0.000100    Time 0.010296    \n",
            "--- validate (epoch=61250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [61250][    1/    1]    Loss 0.000057    \n",
            "==> Loss: 0.0000571\n",
            "\n",
            "==> Best [MSE: 0.0000446   Sparsity:0.00   NNZ-Params: 20672 on epoch: 60994]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=63000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [63000][    1/    1]    Overall Loss 0.000069    Objective Loss 0.000069    LR 0.000100    Time 0.008623    \n",
            "--- validate (epoch=63000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [63000][    1/    1]    Loss 0.000064    \n",
            "==> Loss: 0.0000640\n",
            "\n",
            "==> Best [MSE: 0.0000439   Sparsity:0.00   NNZ-Params: 20672 on epoch: 62646]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=64750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [64750][    1/    1]    Overall Loss 0.000044    Objective Loss 0.000044    LR 0.000100    Time 0.008889    \n",
            "--- validate (epoch=64750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [64750][    1/    1]    Loss 0.000044    \n",
            "==> Loss: 0.0000439\n",
            "\n",
            "==> Best [MSE: 0.0000431   Sparsity:0.00   NNZ-Params: 20672 on epoch: 64678]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=66500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [66500][    1/    1]    Overall Loss 0.000044    Objective Loss 0.000044    LR 0.000100    Time 0.008832    \n",
            "--- validate (epoch=66500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [66500][    1/    1]    Loss 0.000043    \n",
            "==> Loss: 0.0000433\n",
            "\n",
            "==> Best [MSE: 0.0000430   Sparsity:0.00   NNZ-Params: 20672 on epoch: 65710]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=68250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [68250][    1/    1]    Overall Loss 0.000073    Objective Loss 0.000073    LR 0.000100    Time 0.008800    \n",
            "--- validate (epoch=68250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [68250][    1/    1]    Loss 0.000076    \n",
            "==> Loss: 0.0000764\n",
            "\n",
            "==> Best [MSE: 0.0000424   Sparsity:0.00   NNZ-Params: 20672 on epoch: 67949]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=70000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [70000][    1/    1]    Overall Loss 0.000054    Objective Loss 0.000054    LR 0.000100    Time 0.008715    \n",
            "--- validate (epoch=70000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [70000][    1/    1]    Loss 0.000049    \n",
            "==> Loss: 0.0000486\n",
            "\n",
            "==> Best [MSE: 0.0000420   Sparsity:0.00   NNZ-Params: 20672 on epoch: 69869]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=71750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [71750][    1/    1]    Overall Loss 0.000042    Objective Loss 0.000042    LR 0.000100    Time 0.008657    \n",
            "--- validate (epoch=71750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [71750][    1/    1]    Loss 0.000042    \n",
            "==> Loss: 0.0000418\n",
            "\n",
            "==> Best [MSE: 0.0000416   Sparsity:0.00   NNZ-Params: 20672 on epoch: 71326]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=73500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [73500][    1/    1]    Overall Loss 0.000060    Objective Loss 0.000060    LR 0.000100    Time 0.009929    \n",
            "--- validate (epoch=73500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [73500][    1/    1]    Loss 0.000053    \n",
            "==> Loss: 0.0000529\n",
            "\n",
            "==> Best [MSE: 0.0000415   Sparsity:0.00   NNZ-Params: 20672 on epoch: 72308]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=75250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [75250][    1/    1]    Overall Loss 0.000048    Objective Loss 0.000048    LR 0.000100    Time 0.008643    \n",
            "--- validate (epoch=75250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [75250][    1/    1]    Loss 0.000047    \n",
            "==> Loss: 0.0000470\n",
            "\n",
            "==> Best [MSE: 0.0000411   Sparsity:0.00   NNZ-Params: 20672 on epoch: 74884]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=77000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [77000][    1/    1]    Overall Loss 0.000047    Objective Loss 0.000047    LR 0.000100    Time 0.008879    \n",
            "--- validate (epoch=77000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [77000][    1/    1]    Loss 0.000048    \n",
            "==> Loss: 0.0000480\n",
            "\n",
            "==> Best [MSE: 0.0000410   Sparsity:0.00   NNZ-Params: 20672 on epoch: 76943]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=78750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [78750][    1/    1]    Overall Loss 0.000050    Objective Loss 0.000050    LR 0.000100    Time 0.008789    \n",
            "--- validate (epoch=78750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [78750][    1/    1]    Loss 0.000049    \n",
            "==> Loss: 0.0000489\n",
            "\n",
            "==> Best [MSE: 0.0000406   Sparsity:0.00   NNZ-Params: 20672 on epoch: 77916]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=80500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [80500][    1/    1]    Overall Loss 0.000051    Objective Loss 0.000051    LR 0.000100    Time 0.008743    \n",
            "--- validate (epoch=80500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [80500][    1/    1]    Loss 0.000051    \n",
            "==> Loss: 0.0000514\n",
            "\n",
            "==> Best [MSE: 0.0000403   Sparsity:0.00   NNZ-Params: 20672 on epoch: 80356]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=82250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [82250][    1/    1]    Overall Loss 0.000041    Objective Loss 0.000041    LR 0.000100    Time 0.008887    \n",
            "--- validate (epoch=82250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [82250][    1/    1]    Loss 0.000041    \n",
            "==> Loss: 0.0000410\n",
            "\n",
            "==> Best [MSE: 0.0000399   Sparsity:0.00   NNZ-Params: 20672 on epoch: 82022]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=84000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [84000][    1/    1]    Overall Loss 0.000049    Objective Loss 0.000049    LR 0.000100    Time 0.008982    \n",
            "--- validate (epoch=84000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [84000][    1/    1]    Loss 0.000049    \n",
            "==> Loss: 0.0000489\n",
            "\n",
            "==> Best [MSE: 0.0000397   Sparsity:0.00   NNZ-Params: 20672 on epoch: 83365]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=85750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [85750][    1/    1]    Overall Loss 0.000045    Objective Loss 0.000045    LR 0.000100    Time 0.009256    \n",
            "--- validate (epoch=85750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [85750][    1/    1]    Loss 0.000045    \n",
            "==> Loss: 0.0000451\n",
            "\n",
            "==> Best [MSE: 0.0000396   Sparsity:0.00   NNZ-Params: 20672 on epoch: 84631]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=87500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [87500][    1/    1]    Overall Loss 0.000053    Objective Loss 0.000053    LR 0.000100    Time 0.008871    \n",
            "--- validate (epoch=87500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [87500][    1/    1]    Loss 0.000059    \n",
            "==> Loss: 0.0000591\n",
            "\n",
            "==> Best [MSE: 0.0000393   Sparsity:0.00   NNZ-Params: 20672 on epoch: 86634]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=89250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [89250][    1/    1]    Overall Loss 0.000043    Objective Loss 0.000043    LR 0.000100    Time 0.008775    \n",
            "--- validate (epoch=89250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [89250][    1/    1]    Loss 0.000041    \n",
            "==> Loss: 0.0000411\n",
            "\n",
            "==> Best [MSE: 0.0000391   Sparsity:0.00   NNZ-Params: 20672 on epoch: 88740]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=91000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [91000][    1/    1]    Overall Loss 0.000058    Objective Loss 0.000058    LR 0.000100    Time 0.008907    \n",
            "--- validate (epoch=91000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [91000][    1/    1]    Loss 0.000061    \n",
            "==> Loss: 0.0000614\n",
            "\n",
            "==> Best [MSE: 0.0000391   Sparsity:0.00   NNZ-Params: 20672 on epoch: 88740]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=92750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [92750][    1/    1]    Overall Loss 0.000058    Objective Loss 0.000058    LR 0.000100    Time 0.008940    \n",
            "--- validate (epoch=92750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [92750][    1/    1]    Loss 0.000058    \n",
            "==> Loss: 0.0000577\n",
            "\n",
            "==> Best [MSE: 0.0000389   Sparsity:0.00   NNZ-Params: 20672 on epoch: 91426]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=94500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [94500][    1/    1]    Overall Loss 0.000041    Objective Loss 0.000041    LR 0.000100    Time 0.009466    \n",
            "--- validate (epoch=94500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [94500][    1/    1]    Loss 0.000042    \n",
            "==> Loss: 0.0000421\n",
            "\n",
            "==> Best [MSE: 0.0000388   Sparsity:0.00   NNZ-Params: 20672 on epoch: 93636]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=96250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [96250][    1/    1]    Overall Loss 0.000046    Objective Loss 0.000046    LR 0.000100    Time 0.009034    \n",
            "--- validate (epoch=96250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [96250][    1/    1]    Loss 0.000050    \n",
            "==> Loss: 0.0000500\n",
            "\n",
            "==> Best [MSE: 0.0000385   Sparsity:0.00   NNZ-Params: 20672 on epoch: 95369]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=98000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [98000][    1/    1]    Overall Loss 0.000040    Objective Loss 0.000040    LR 0.000100    Time 0.008915    \n",
            "--- validate (epoch=98000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [98000][    1/    1]    Loss 0.000040    \n",
            "==> Loss: 0.0000398\n",
            "\n",
            "==> Best [MSE: 0.0000383   Sparsity:0.00   NNZ-Params: 20672 on epoch: 97539]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=99750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [99750][    1/    1]    Overall Loss 0.000038    Objective Loss 0.000038    LR 0.000100    Time 0.009061    \n",
            "--- validate (epoch=99750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [99750][    1/    1]    Loss 0.000038    \n",
            "==> Loss: 0.0000383\n",
            "\n",
            "==> Best [MSE: 0.0000381   Sparsity:0.00   NNZ-Params: 20672 on epoch: 99285]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=101500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [101500][    1/    1]    Overall Loss 0.000074    Objective Loss 0.000074    LR 0.000100    Time 0.010542    \n",
            "--- validate (epoch=101500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [101500][    1/    1]    Loss 0.000085    \n",
            "==> Loss: 0.0000846\n",
            "\n",
            "==> Best [MSE: 0.0000381   Sparsity:0.00   NNZ-Params: 20672 on epoch: 99285]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=103250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [103250][    1/    1]    Overall Loss 0.000055    Objective Loss 0.000055    LR 0.000100    Time 0.009132    \n",
            "--- validate (epoch=103250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [103250][    1/    1]    Loss 0.000060    \n",
            "==> Loss: 0.0000598\n",
            "\n",
            "==> Best [MSE: 0.0000380   Sparsity:0.00   NNZ-Params: 20672 on epoch: 102464]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=105000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [105000][    1/    1]    Overall Loss 0.000047    Objective Loss 0.000047    LR 0.000100    Time 0.009189    \n",
            "--- validate (epoch=105000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [105000][    1/    1]    Loss 0.000047    \n",
            "==> Loss: 0.0000466\n",
            "\n",
            "==> Best [MSE: 0.0000379   Sparsity:0.00   NNZ-Params: 20672 on epoch: 104062]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=106750)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [106750][    1/    1]    Overall Loss 0.000050    Objective Loss 0.000050    LR 0.000100    Time 0.011929    \n",
            "--- validate (epoch=106750)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [106750][    1/    1]    Loss 0.000048    \n",
            "==> Loss: 0.0000479\n",
            "\n",
            "==> Best [MSE: 0.0000379   Sparsity:0.00   NNZ-Params: 20672 on epoch: 104062]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=108500)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [108500][    1/    1]    Overall Loss 0.000110    Objective Loss 0.000110    LR 0.000100    Time 0.010479    \n",
            "--- validate (epoch=108500)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [108500][    1/    1]    Loss 0.000112    \n",
            "==> Loss: 0.0001116\n",
            "\n",
            "==> Best [MSE: 0.0000376   Sparsity:0.00   NNZ-Params: 20672 on epoch: 108263]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=110250)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [110250][    1/    1]    Overall Loss 0.000042    Objective Loss 0.000042    LR 0.000100    Time 0.009902    \n",
            "--- validate (epoch=110250)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [110250][    1/    1]    Loss 0.000041    \n",
            "==> Loss: 0.0000413\n",
            "\n",
            "==> Best [MSE: 0.0000376   Sparsity:0.00   NNZ-Params: 20672 on epoch: 109182]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n",
            "\n",
            "\n",
            "--- train (epoch=112000)-----------\n",
            "Training epoch: 1 samples (1 per mini-batch)\n",
            "Epoch: [112000][    1/    1]    Overall Loss 0.000057    Objective Loss 0.000057    LR 0.000100    Time 0.008964    \n",
            "--- validate (epoch=112000)-----------\n",
            "1 samples (1 per mini-batch)\n",
            "Epoch: [112000][    1/    1]    Loss 0.000059    \n",
            "==> Loss: 0.0000590\n",
            "\n",
            "==> Best [MSE: 0.0000375   Sparsity:0.00   NNZ-Params: 20672 on epoch: 110781]\n",
            "Saving checkpoint to: /content/drive/MyDrive/Siren Deep Learning Analyses/results/cameramen/___2020.11.27-162900/_checkpoint.pth.tar\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}