


% Compression Techniques (2)
% ---------------------------------------------- 
\begin{frame}
    \frametitle{Compression Techniques (2)}
        \textbf{Deep Neural Network Models Compression Techniques} can be devided into a given number of different classes or categories depending on
        the approaches they follow. In particular we can devide such methods, without the claim of being for sure exhaustive but at least quite accurate,
        as follows:

        \begin{columns}

            \column{0.25\textwidth}
                \textbf{Weight Sharing}
                \begin{itemize}
                \item Cluster-based Weight Sharing
                \item Learning Weight Sharing
                \item Weight Sharing in Large Archs
                % \item Reusing layers Recursively
                \item ...
                \end{itemize}


            \column{0.25\textwidth}
                \textbf{Network Pruning}
                \begin{itemize}
                \item Pruning via Weights Regularization
                \item Pruning via Loss Sensitivity
                \item Structured Pruning
                % \item Search Based Pruning
                % \item Pruning Before Training
                \item ...
                \end{itemize}


            \column{0.25\textwidth}
                \textbf{Quantization}
                \begin{itemize}
                \item Adaptive Range and Clipping
                \item Linear Range Quant
                \item DoReFa Net Quant
                \item WRPN Net Quant
                \item ...
                \end{itemize}


            \column{0.25\textwidth}
            \textbf{Knowledge Distillation}
                \begin{itemize}
                \item Recurrent (Autoregressive) NNs
                \item Transformer-based (Non-Autoregressive) NNs
                \item Data Free KD
                % \item Ensemble-based KD
                % \item Reinforced Learning KD
                \item ...
                \end{itemize}

        \end{columns}
\end{frame}
